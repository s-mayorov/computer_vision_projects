{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitfastaivenvd88b628ba05549c1abc1295c7d007138",
   "display_name": "Python 3.8.2 64-bit ('fastai': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The cat ate the cheese\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"She read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"The dog loves art\".lower().split(), [\"DET\", \"NN\", \"V\", \"NN\"]),\n",
    "    (\"The elephant answers the phone\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "word2idx = {}\n",
    "for line, tags in training_data:\n",
    "    for word in line:\n",
    "        if word not in word2idx:\n",
    "           word2idx[word] = len(word2idx)\n",
    "\n",
    "tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'the': 0, 'cat': 1, 'ate': 2, 'cheese': 3, 'she': 4, 'read': 5, 'that': 6, 'book': 7, 'dog': 8, 'loves': 9, 'art': 10, 'elephant': 11, 'answers': 12, 'phone': 13}\n"
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert sequence of words into tensor of correspondig indexes\n",
    "def prepare_sequence(seq, to_idx):\n",
    "    idxs = np.array([to_idx[w] for w in seq])\n",
    "    return torch.from_numpy(idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([ 0,  8, 12,  0, 13])\n"
    }
   ],
   "source": [
    "print(prepare_sequence([\"the\", \"dog\", \"answers\", \"the\", \"phone\"], word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "\n",
    "        tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_outputs, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding dimension defines the size of our word vectors\n",
    "# for our simple vocabulary and training set, we will keep these small\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx))\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-1.1056, -1.0358, -1.1583],\n        [-1.2082, -1.0120, -1.0854],\n        [-1.1566, -0.9996, -1.1475],\n        [-1.1029, -1.0392, -1.1573],\n        [-1.0837, -1.0457, -1.1705]], grad_fn=<LogSoftmaxBackward>)\n"
    }
   ],
   "source": [
    "# test model BEFORE training to see later that it learns something and performs better\n",
    "test_sentense = \"The cheese loves the elephant\".lower().split()\n",
    "\n",
    "inputs = prepare_sequence(test_sentense, word2idx)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1, 1, 1, 1, 1])\n"
    }
   ],
   "source": [
    "_, predicted_tags = torch.max(tag_scores, dim=1)\n",
    "print(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 19 Loss 0.032280809711664915\nEpoch 39 Loss 0.02875902969390154\nEpoch 59 Loss 0.02587804989889264\nEpoch 79 Loss 0.023482127115130424\nEpoch 99 Loss 0.021461260970681906\nEpoch 119 Loss 0.019735854817554355\nEpoch 139 Loss 0.018247031373903155\nEpoch 159 Loss 0.01695034746080637\nEpoch 179 Loss 0.015811806311830878\nEpoch 199 Loss 0.014804745558649302\nEpoch 219 Loss 0.013908273307606578\nEpoch 239 Loss 0.013105457415804267\nEpoch 259 Loss 0.012382772751152515\nEpoch 279 Loss 0.011729001766070724\nEpoch 299 Loss 0.01113503216765821\n"
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for sentence, tags in training_data:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, word2idx)\n",
    "        targets = prepare_sequence(tags, tag2idx)\n",
    "\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%20==19:\n",
    "        print(\"Epoch {} Loss {}\".format(epoch, epoch_loss/len(training_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-6.9385e-03, -4.9743e+00, -1.3731e+01],\n        [-6.2323e+00, -2.4220e-03, -7.6969e+00],\n        [-1.2311e+01, -4.0028e+00, -1.8438e-02],\n        [-3.6149e-03, -5.6370e+00, -1.0009e+01],\n        [-6.6501e+00, -9.4885e-02, -2.4166e+00]], grad_fn=<LogSoftmaxBackward>)\n"
    }
   ],
   "source": [
    "# test model with same test sentence as before\n",
    "test_sentense = \"The cheese loves the elephant\".lower().split()\n",
    "\n",
    "inputs = prepare_sequence(test_sentense, word2idx)\n",
    "tag_scores = model(inputs)\n",
    "print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0, 1, 2, 0, 1])\n"
    }
   ],
   "source": [
    "_, predicted_tags = torch.max(tag_scores, dim=1)\n",
    "print(predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}