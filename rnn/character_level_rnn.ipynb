{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitfastaivenvd88b628ba05549c1abc1295c7d007138",
   "display_name": "Python 3.8.2 64-bit ('fastai': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/anna.txt\", 'r') as f:\n",
    "    data = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(data)) # unique chars set\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 4, 36,  1, 52, 66, 21, 14, 69,  6, 78, 78, 78, 74,  1, 52, 52, 61,\n       69, 56,  1,  8, 55, 32, 55, 21, 72, 69,  1, 14, 21, 69,  1, 32, 32,\n       69,  1, 32, 55, 51, 21, 41, 69, 21,  3, 21, 14, 61, 69, 13, 59, 36,\n        1, 52, 52, 61, 69, 56,  1,  8, 55, 32, 61, 69, 55, 72, 69, 13, 59,\n       36,  1, 52, 52, 61, 69, 55, 59, 69, 55, 66, 72, 69, 35, 47, 59, 78,\n       47,  1, 61, 37, 78, 78, 40,  3, 21, 14, 61, 66, 36, 55, 59])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    # Get the number of characters per batch\n",
    "    batch_size = n_seqs * n_steps\n",
    "    \n",
    "    ## Get the number of batches we can make\n",
    "    n_batches = len(arr)//batch_size\n",
    "\n",
    "    ## Keep only enough characters to make full batches\n",
    "    arr = arr[:batch_size*n_batches]\n",
    "\n",
    "    ## Reshape into batch_size rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x\n [[ 4 36  1 52 66 21 14 69  6 78]\n [69  1  8 69 59 35 66 69 17 35]\n [ 3 55 59 37 78 78 33  5 21 72]\n [59 69 24 13 14 55 59 17 69 36]\n [69 55 66 69 55 72 70 69 72 55]\n [69 31 66 69 47  1 72 78 35 59]\n [36 21 59 69 57 35  8 21 69 56]\n [41 69 50 13 66 69 59 35 47 69]\n [66 69 55 72 59 53 66 37 69 54]\n [69 72  1 55 24 69 66 35 69 36]]\n\ny\n [[36  1 52 66 21 14 69  6 78 78]\n [ 1  8 69 59 35 66 69 17 35 55]\n [55 59 37 78 78 33  5 21 72 70]\n [69 24 13 14 55 59 17 69 36 55]\n [55 66 69 55 72 70 69 72 55 14]\n [31 66 69 47  1 72 78 35 59 32]\n [21 59 69 57 35  8 21 69 56 35]\n [69 50 13 66 69 59 35 47 69 72]\n [69 55 72 59 53 66 37 69 54 36]\n [72  1 55 24 69 66 35 69 36 21]]\n"
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print(\"x\\n\", x[:10, :10])\n",
    "print(\"\\ny\\n\", y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, (h, c)\n",
    "\n",
    "    def predict(self, char, h=None, cuda=True, top_k=None):\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "\n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot(x, len(self.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "\n",
    "        if top_k is None:\n",
    "            top_ch = self.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "    \n",
    "    def init_hidden(self, n_seqs):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=True, print_every=10):\n",
    "    net.train()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if cuda: net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            x = one_hot(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            h = tuple([each.data for each in h])\n",
    "            net.zero_grad()\n",
    "\n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CharRNN(\n  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=83, bias=True)\n)\n"
    }
   ],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s: 2.0892\nEpoch: 3/25... Step: 280... Loss: 2.0230... Val Loss: 2.0764\nEpoch: 3/25... Step: 290... Loss: 2.0142... Val Loss: 2.0715\nEpoch: 3/25... Step: 300... Loss: 1.9707... Val Loss: 2.0479\nEpoch: 3/25... Step: 310... Loss: 1.9550... Val Loss: 2.0414\nEpoch: 3/25... Step: 320... Loss: 1.9291... Val Loss: 2.0328\nEpoch: 3/25... Step: 330... Loss: 1.9045... Val Loss: 2.0214\nEpoch: 3/25... Step: 340... Loss: 1.9532... Val Loss: 2.0127\nEpoch: 3/25... Step: 350... Loss: 1.9102... Val Loss: 1.9982\nEpoch: 3/25... Step: 360... Loss: 1.8532... Val Loss: 2.0246\nEpoch: 3/25... Step: 370... Loss: 1.8925... Val Loss: 1.9748\nEpoch: 3/25... Step: 380... Loss: 1.8950... Val Loss: 2.0188\nEpoch: 3/25... Step: 390... Loss: 1.8572... Val Loss: 2.0122\nEpoch: 3/25... Step: 400... Loss: 1.8464... Val Loss: 1.9541\nEpoch: 3/25... Step: 410... Loss: 1.8473... Val Loss: 1.9934\nEpoch: 4/25... Step: 420... Loss: 1.8510... Val Loss: 1.9375\nEpoch: 4/25... Step: 430... Loss: 1.8339... Val Loss: 1.9280\nEpoch: 4/25... Step: 440... Loss: 1.8314... Val Loss: 1.9111\nEpoch: 4/25... Step: 450... Loss: 1.7687... Val Loss: 1.9041\nEpoch: 4/25... Step: 460... Loss: 1.7642... Val Loss: 1.8941\nEpoch: 4/25... Step: 470... Loss: 1.8073... Val Loss: 1.8937\nEpoch: 4/25... Step: 480... Loss: 1.7927... Val Loss: 1.8859\nEpoch: 4/25... Step: 490... Loss: 1.7916... Val Loss: 1.8761\nEpoch: 4/25... Step: 500... Loss: 1.7811... Val Loss: 1.8669\nEpoch: 4/25... Step: 510... Loss: 1.7589... Val Loss: 1.8540\nEpoch: 4/25... Step: 520... Loss: 1.7697... Val Loss: 1.8446\nEpoch: 4/25... Step: 530... Loss: 1.7478... Val Loss: 1.8414\nEpoch: 4/25... Step: 540... Loss: 1.7117... Val Loss: 1.8348\nEpoch: 4/25... Step: 550... Loss: 1.7770... Val Loss: 1.8312\nEpoch: 5/25... Step: 560... Loss: 1.7138... Val Loss: 1.8246\nEpoch: 5/25... Step: 570... Loss: 1.7138... Val Loss: 1.8196\nEpoch: 5/25... Step: 580... Loss: 1.6945... Val Loss: 1.8084\nEpoch: 5/25... Step: 590... Loss: 1.7037... Val Loss: 1.7968\nEpoch: 5/25... Step: 600... Loss: 1.6854... Val Loss: 1.7988\nEpoch: 5/25... Step: 610... Loss: 1.6633... Val Loss: 1.7823\nEpoch: 5/25... Step: 620... Loss: 1.6864... Val Loss: 1.7793\nEpoch: 5/25... Step: 630... Loss: 1.6933... Val Loss: 1.7682\nEpoch: 5/25... Step: 640... Loss: 1.6504... Val Loss: 1.7637\nEpoch: 5/25... Step: 650... Loss: 1.6633... Val Loss: 1.7646\nEpoch: 5/25... Step: 660... Loss: 1.6368... Val Loss: 1.7676\nEpoch: 5/25... Step: 670... Loss: 1.6694... Val Loss: 1.7565\nEpoch: 5/25... Step: 680... Loss: 1.6660... Val Loss: 1.7523\nEpoch: 5/25... Step: 690... Loss: 1.6352... Val Loss: 1.7449\nEpoch: 6/25... Step: 700... Loss: 1.6372... Val Loss: 1.7442\nEpoch: 6/25... Step: 710... Loss: 1.6284... Val Loss: 1.7405\nEpoch: 6/25... Step: 720... Loss: 1.6189... Val Loss: 1.7318\nEpoch: 6/25... Step: 730... Loss: 1.6222... Val Loss: 1.7310\nEpoch: 6/25... Step: 740... Loss: 1.6110... Val Loss: 1.7401\nEpoch: 6/25... Step: 750... Loss: 1.5802... Val Loss: 1.7325\nEpoch: 6/25... Step: 760... Loss: 1.6169... Val Loss: 1.7249\nEpoch: 6/25... Step: 770... Loss: 1.6022... Val Loss: 1.7077\nEpoch: 6/25... Step: 780... Loss: 1.5955... Val Loss: 1.7131\nEpoch: 6/25... Step: 790... Loss: 1.5638... Val Loss: 1.7112\nEpoch: 6/25... Step: 800... Loss: 1.6024... Val Loss: 1.6954\nEpoch: 6/25... Step: 810... Loss: 1.5705... Val Loss: 1.7004\nEpoch: 6/25... Step: 820... Loss: 1.5344... Val Loss: 1.7037\nEpoch: 6/25... Step: 830... Loss: 1.5891... Val Loss: 1.6883\nEpoch: 7/25... Step: 840... Loss: 1.5422... Val Loss: 1.6859\nEpoch: 7/25... Step: 850... Loss: 1.5606... Val Loss: 1.6727\nEpoch: 7/25... Step: 860... Loss: 1.5368... Val Loss: 1.6732\nEpoch: 7/25... Step: 870... Loss: 1.5500... Val Loss: 1.6735\nEpoch: 7/25... Step: 880... Loss: 1.5637... Val Loss: 1.6761\nEpoch: 7/25... Step: 890... Loss: 1.5585... Val Loss: 1.6665\nEpoch: 7/25... Step: 900... Loss: 1.5482... Val Loss: 1.6608\nEpoch: 7/25... Step: 910... Loss: 1.4947... Val Loss: 1.6537\nEpoch: 7/25... Step: 920... Loss: 1.5362... Val Loss: 1.6617\nEpoch: 7/25... Step: 930... Loss: 1.5208... Val Loss: 1.6601\nEpoch: 7/25... Step: 940... Loss: 1.5336... Val Loss: 1.6490\nEpoch: 7/25... Step: 950... Loss: 1.5353... Val Loss: 1.6474\nEpoch: 7/25... Step: 960... Loss: 1.5324... Val Loss: 1.6416\nEpoch: 7/25... Step: 970... Loss: 1.5414... Val Loss: 1.6478\nEpoch: 8/25... Step: 980... Loss: 1.5089... Val Loss: 1.6450\nEpoch: 8/25... Step: 990... Loss: 1.5052... Val Loss: 1.6321\nEpoch: 8/25... Step: 1000... Loss: 1.5050... Val Loss: 1.6322\nEpoch: 8/25... Step: 1010... Loss: 1.5326... Val Loss: 1.6249\nEpoch: 8/25... Step: 1020... Loss: 1.5138... Val Loss: 1.6250\nEpoch: 8/25... Step: 1030... Loss: 1.5081... Val Loss: 1.6309\nEpoch: 8/25... Step: 1040... Loss: 1.5014... Val Loss: 1.6228\nEpoch: 8/25... Step: 1050... Loss: 1.4801... Val Loss: 1.6200\nEpoch: 8/25... Step: 1060... Loss: 1.4860... Val Loss: 1.6171\nEpoch: 8/25... Step: 1070... Loss: 1.4916... Val Loss: 1.6141\nEpoch: 8/25... Step: 1080... Loss: 1.4833... Val Loss: 1.6108\nEpoch: 8/25... Step: 1090... Loss: 1.4697... Val Loss: 1.6154\nEpoch: 8/25... Step: 1100... Loss: 1.4793... Val Loss: 1.6074\nEpoch: 8/25... Step: 1110... Loss: 1.4746... Val Loss: 1.6020\nEpoch: 9/25... Step: 1120... Loss: 1.4696... Val Loss: 1.5981\nEpoch: 9/25... Step: 1130... Loss: 1.4738... Val Loss: 1.5938\nEpoch: 9/25... Step: 1140... Loss: 1.4841... Val Loss: 1.5928\nEpoch: 9/25... Step: 1150... Loss: 1.4985... Val Loss: 1.5941\nEpoch: 9/25... Step: 1160... Loss: 1.4528... Val Loss: 1.5931\nEpoch: 9/25... Step: 1170... Loss: 1.4638... Val Loss: 1.5928\nEpoch: 9/25... Step: 1180... Loss: 1.4621... Val Loss: 1.5868\nEpoch: 9/25... Step: 1190... Loss: 1.4934... Val Loss: 1.5806\nEpoch: 9/25... Step: 1200... Loss: 1.4329... Val Loss: 1.5800\nEpoch: 9/25... Step: 1210... Loss: 1.4419... Val Loss: 1.5836\nEpoch: 9/25... Step: 1220... Loss: 1.4564... Val Loss: 1.5798\nEpoch: 9/25... Step: 1230... Loss: 1.4385... Val Loss: 1.5818\nEpoch: 9/25... Step: 1240... Loss: 1.4412... Val Loss: 1.5702\nEpoch: 9/25... Step: 1250... Loss: 1.4494... Val Loss: 1.5748\nEpoch: 10/25... Step: 1260... Loss: 1.4680... Val Loss: 1.5722\nEpoch: 10/25... Step: 1270... Loss: 1.4403... Val Loss: 1.5697\nEpoch: 10/25... Step: 1280... Loss: 1.4670... Val Loss: 1.5677\nEpoch: 10/25... Step: 1290... Loss: 1.4422... Val Loss: 1.5646\nEpoch: 10/25... Step: 1300... Loss: 1.4318... Val Loss: 1.5678\nEpoch: 10/25... Step: 1310... Loss: 1.4483... Val Loss: 1.5626\nEpoch: 10/25... Step: 1320... Loss: 1.4216... Val Loss: 1.5641\nEpoch: 10/25... Step: 1330... Loss: 1.4195... Val Loss: 1.5594\nEpoch: 10/25... Step: 1340... Loss: 1.4046... Val Loss: 1.5550\nEpoch: 10/25... Step: 1350... Loss: 1.3973... Val Loss: 1.5645\nEpoch: 10/25... Step: 1360... Loss: 1.4117... Val Loss: 1.5629\nEpoch: 10/25... Step: 1370... Loss: 1.3920... Val Loss: 1.5600\nEpoch: 10/25... Step: 1380... Loss: 1.4288... Val Loss: 1.5492\nEpoch: 10/25... Step: 1390... Loss: 1.3955... Val Loss: 1.5471\nEpoch: 11/25... Step: 1400... Loss: 1.4433... Val Loss: 1.5587\nEpoch: 11/25... Step: 1410... Loss: 1.4669... Val Loss: 1.5492\nEpoch: 11/25... Step: 1420... Loss: 1.4445... Val Loss: 1.5418\nEpoch: 11/25... Step: 1430... Loss: 1.4169... Val Loss: 1.5447\nEpoch: 11/25... Step: 1440... Loss: 1.4148... Val Loss: 1.5468\nEpoch: 11/25... Step: 1450... Loss: 1.3659... Val Loss: 1.5409\nEpoch: 11/25... Step: 1460... Loss: 1.3961... Val Loss: 1.5368\nEpoch: 11/25... Step: 1470... Loss: 1.3841... Val Loss: 1.5449\nEpoch: 11/25... Step: 1480... Loss: 1.3990... Val Loss: 1.5362\nEpoch: 11/25... Step: 1490... Loss: 1.3856... Val Loss: 1.5312\nEpoch: 11/25... Step: 1500... Loss: 1.3815... Val Loss: 1.5377\nEpoch: 11/25... Step: 1510... Loss: 1.3669... Val Loss: 1.5432\nEpoch: 11/25... Step: 1520... Loss: 1.3994... Val Loss: 1.5284\nEpoch: 12/25... Step: 1530... Loss: 1.4561... Val Loss: 1.5287\nEpoch: 12/25... Step: 1540... Loss: 1.4075... Val Loss: 1.5301\nEpoch: 12/25... Step: 1550... Loss: 1.4119... Val Loss: 1.5354\nEpoch: 12/25... Step: 1560... Loss: 1.4313... Val Loss: 1.5286\nEpoch: 12/25... Step: 1570... Loss: 1.3684... Val Loss: 1.5250\nEpoch: 12/25... Step: 1580... Loss: 1.3488... Val Loss: 1.5207\nEpoch: 12/25... Step: 1590... Loss: 1.3411... Val Loss: 1.5256\nEpoch: 12/25... Step: 1600... Loss: 1.3703... Val Loss: 1.5290\nEpoch: 12/25... Step: 1610... Loss: 1.3652... Val Loss: 1.5201\nEpoch: 12/25... Step: 1620... Loss: 1.3642... Val Loss: 1.5200\nEpoch: 12/25... Step: 1630... Loss: 1.3862... Val Loss: 1.5240\nEpoch: 12/25... Step: 1640... Loss: 1.3572... Val Loss: 1.5218\nEpoch: 12/25... Step: 1650... Loss: 1.3368... Val Loss: 1.5249\nEpoch: 12/25... Step: 1660... Loss: 1.3931... Val Loss: 1.5185\nEpoch: 13/25... Step: 1670... Loss: 1.3619... Val Loss: 1.5149\nEpoch: 13/25... Step: 1680... Loss: 1.3742... Val Loss: 1.5125\nEpoch: 13/25... Step: 1690... Loss: 1.3578... Val Loss: 1.5111\nEpoch: 13/25... Step: 1700... Loss: 1.3530... Val Loss: 1.5155\nEpoch: 13/25... Step: 1710... Loss: 1.3427... Val Loss: 1.5145\nEpoch: 13/25... Step: 1720... Loss: 1.3387... Val Loss: 1.5134\nEpoch: 13/25... Step: 1730... Loss: 1.3787... Val Loss: 1.5155\nEpoch: 13/25... Step: 1740... Loss: 1.3577... Val Loss: 1.5077\nEpoch: 13/25... Step: 1750... Loss: 1.3178... Val Loss: 1.5088\nEpoch: 13/25... Step: 1760... Loss: 1.3494... Val Loss: 1.5059\nEpoch: 13/25... Step: 1770... Loss: 1.3689... Val Loss: 1.5060\nEpoch: 13/25... Step: 1780... Loss: 1.3324... Val Loss: 1.5079\nEpoch: 13/25... Step: 1790... Loss: 1.3364... Val Loss: 1.5023\nEpoch: 13/25... Step: 1800... Loss: 1.3411... Val Loss: 1.4997\nEpoch: 14/25... Step: 1810... Loss: 1.3609... Val Loss: 1.5034\nEpoch: 14/25... Step: 1820... Loss: 1.3405... Val Loss: 1.5030\nEpoch: 14/25... Step: 1830... Loss: 1.3620... Val Loss: 1.5051\nEpoch: 14/25... Step: 1840... Loss: 1.3175... Val Loss: 1.5058\nEpoch: 14/25... Step: 1850... Loss: 1.2991... Val Loss: 1.5007\nEpoch: 14/25... Step: 1860... Loss: 1.3480... Val Loss: 1.5014\nEpoch: 14/25... Step: 1870... Loss: 1.3551... Val Loss: 1.5024\nEpoch: 14/25... Step: 1880... Loss: 1.3402... Val Loss: 1.5029\nEpoch: 14/25... Step: 1890... Loss: 1.3573... Val Loss: 1.5016\nEpoch: 14/25... Step: 1900... Loss: 1.3495... Val Loss: 1.4989\nEpoch: 14/25... Step: 1910... Loss: 1.3430... Val Loss: 1.4921\nEpoch: 14/25... Step: 1920... Loss: 1.3449... Val Loss: 1.4981\nEpoch: 14/25... Step: 1930... Loss: 1.3026... Val Loss: 1.4990\nEpoch: 14/25... Step: 1940... Loss: 1.3725... Val Loss: 1.4947\nEpoch: 15/25... Step: 1950... Loss: 1.3221... Val Loss: 1.4904\nEpoch: 15/25... Step: 1960... Loss: 1.3315... Val Loss: 1.4895\nEpoch: 15/25... Step: 1970... Loss: 1.3231... Val Loss: 1.4896\nEpoch: 15/25... Step: 1980... Loss: 1.3139... Val Loss: 1.4861\nEpoch: 15/25... Step: 1990... Loss: 1.3176... Val Loss: 1.4832\nEpoch: 15/25... Step: 2000... Loss: 1.3011... Val Loss: 1.4880\nEpoch: 15/25... Step: 2010... Loss: 1.3224... Val Loss: 1.4835\nEpoch: 15/25... Step: 2020... Loss: 1.3582... Val Loss: 1.4900\nEpoch: 15/25... Step: 2030... Loss: 1.3079... Val Loss: 1.4885\nEpoch: 15/25... Step: 2040... Loss: 1.3267... Val Loss: 1.4850\nEpoch: 15/25... Step: 2050... Loss: 1.3171... Val Loss: 1.4798\nEpoch: 15/25... Step: 2060... Loss: 1.3169... Val Loss: 1.4851\nEpoch: 15/25... Step: 2070... Loss: 1.3292... Val Loss: 1.4839\nEpoch: 15/25... Step: 2080... Loss: 1.3211... Val Loss: 1.4776\nEpoch: 16/25... Step: 2090... Loss: 1.3278... Val Loss: 1.4740\nEpoch: 16/25... Step: 2100... Loss: 1.3177... Val Loss: 1.4820\nEpoch: 16/25... Step: 2110... Loss: 1.3067... Val Loss: 1.4811\nEpoch: 16/25... Step: 2120... Loss: 1.3234... Val Loss: 1.4816\nEpoch: 16/25... Step: 2130... Loss: 1.2991... Val Loss: 1.4778\nEpoch: 16/25... Step: 2140... Loss: 1.2994... Val Loss: 1.4773\nEpoch: 16/25... Step: 2150... Loss: 1.3211... Val Loss: 1.4765\nEpoch: 16/25... Step: 2160... Loss: 1.3161... Val Loss: 1.4834\nEpoch: 16/25... Step: 2170... Loss: 1.2978... Val Loss: 1.4776\nEpoch: 16/25... Step: 2180... Loss: 1.2923... Val Loss: 1.4753\nEpoch: 16/25... Step: 2190... Loss: 1.3154... Val Loss: 1.4699\nEpoch: 16/25... Step: 2200... Loss: 1.2884... Val Loss: 1.4735\nEpoch: 16/25... Step: 2210... Loss: 1.2628... Val Loss: 1.4740\nEpoch: 16/25... Step: 2220... Loss: 1.3088... Val Loss: 1.4657\nEpoch: 17/25... Step: 2230... Loss: 1.2812... Val Loss: 1.4675\nEpoch: 17/25... Step: 2240... Loss: 1.2913... Val Loss: 1.4699\nEpoch: 17/25... Step: 2250... Loss: 1.2857... Val Loss: 1.4651\nEpoch: 17/25... Step: 2260... Loss: 1.2991... Val Loss: 1.4734\nEpoch: 17/25... Step: 2270... Loss: 1.2951... Val Loss: 1.4792\nEpoch: 17/25... Step: 2280... Loss: 1.3015... Val Loss: 1.4701\nEpoch: 17/25... Step: 2290... Loss: 1.2979... Val Loss: 1.4737\nEpoch: 17/25... Step: 2300... Loss: 1.2605... Val Loss: 1.4781\nEpoch: 17/25... Step: 2310... Loss: 1.2883... Val Loss: 1.4693\nEpoch: 17/25... Step: 2320... Loss: 1.2728... Val Loss: 1.4656\nEpoch: 17/25... Step: 2330... Loss: 1.2792... Val Loss: 1.4629\nEpoch: 17/25... Step: 2340... Loss: 1.3058... Val Loss: 1.4659\nEpoch: 17/25... Step: 2350... Loss: 1.3033... Val Loss: 1.4613\nEpoch: 17/25... Step: 2360... Loss: 1.2953... Val Loss: 1.4638\nEpoch: 18/25... Step: 2370... Loss: 1.2698... Val Loss: 1.4603\nEpoch: 18/25... Step: 2380... Loss: 1.2718... Val Loss: 1.4589\nEpoch: 18/25... Step: 2390... Loss: 1.2864... Val Loss: 1.4751\nEpoch: 18/25... Step: 2400... Loss: 1.2967... Val Loss: 1.4714\nEpoch: 18/25... Step: 2410... Loss: 1.3051... Val Loss: 1.4609\nEpoch: 18/25... Step: 2420... Loss: 1.2762... Val Loss: 1.4588\nEpoch: 18/25... Step: 2430... Loss: 1.2849... Val Loss: 1.4536\nEpoch: 18/25... Step: 2440... Loss: 1.2789... Val Loss: 1.4544\nEpoch: 18/25... Step: 2450... Loss: 1.2619... Val Loss: 1.4511\nEpoch: 18/25... Step: 2460... Loss: 1.2867... Val Loss: 1.4619\nEpoch: 18/25... Step: 2470... Loss: 1.2800... Val Loss: 1.4532\nEpoch: 18/25... Step: 2480... Loss: 1.2639... Val Loss: 1.4629\nEpoch: 18/25... Step: 2490... Loss: 1.2568... Val Loss: 1.4528\nEpoch: 18/25... Step: 2500... Loss: 1.2604... Val Loss: 1.4472\nEpoch: 19/25... Step: 2510... Loss: 1.2638... Val Loss: 1.4582\nEpoch: 19/25... Step: 2520... Loss: 1.2721... Val Loss: 1.4624\nEpoch: 19/25... Step: 2530... Loss: 1.2762... Val Loss: 1.4584\nEpoch: 19/25... Step: 2540... Loss: 1.2942... Val Loss: 1.4532\nEpoch: 19/25... Step: 2550... Loss: 1.2724... Val Loss: 1.4480\nEpoch: 19/25... Step: 2560... Loss: 1.2813... Val Loss: 1.4478\nEpoch: 19/25... Step: 2570... Loss: 1.2570... Val Loss: 1.4442\nEpoch: 19/25... Step: 2580... Loss: 1.2944... Val Loss: 1.4477\nEpoch: 19/25... Step: 2590... Loss: 1.2512... Val Loss: 1.4499\nEpoch: 19/25... Step: 2600... Loss: 1.2489... Val Loss: 1.4523\nEpoch: 19/25... Step: 2610... Loss: 1.2613... Val Loss: 1.4428\nEpoch: 19/25... Step: 2620... Loss: 1.2404... Val Loss: 1.4439\nEpoch: 19/25... Step: 2630... Loss: 1.2457... Val Loss: 1.4441\nEpoch: 19/25... Step: 2640... Loss: 1.2586... Val Loss: 1.4392\nEpoch: 20/25... Step: 2650... Loss: 1.2769... Val Loss: 1.4447\nEpoch: 20/25... Step: 2660... Loss: 1.2589... Val Loss: 1.4522\nEpoch: 20/25... Step: 2670... Loss: 1.2749... Val Loss: 1.4491\nEpoch: 20/25... Step: 2680... Loss: 1.2546... Val Loss: 1.4521\nEpoch: 20/25... Step: 2690... Loss: 1.2618... Val Loss: 1.4433\nEpoch: 20/25... Step: 2700... Loss: 1.2667... Val Loss: 1.4447\nEpoch: 20/25... Step: 2710... Loss: 1.2389... Val Loss: 1.4453\nEpoch: 20/25... Step: 2720... Loss: 1.2327... Val Loss: 1.4471\nEpoch: 20/25... Step: 2730... Loss: 1.2271... Val Loss: 1.4442\nEpoch: 20/25... Step: 2740... Loss: 1.2237... Val Loss: 1.4461\nEpoch: 20/25... Step: 2750... Loss: 1.2334... Val Loss: 1.4487\nEpoch: 20/25... Step: 2760... Loss: 1.2202... Val Loss: 1.4418\nEpoch: 20/25... Step: 2770... Loss: 1.2629... Val Loss: 1.4485\nEpoch: 20/25... Step: 2780... Loss: 1.2378... Val Loss: 1.4499\nEpoch: 21/25... Step: 2790... Loss: 1.2788... Val Loss: 1.4446\nEpoch: 21/25... Step: 2800... Loss: 1.2849... Val Loss: 1.4435\nEpoch: 21/25... Step: 2810... Loss: 1.2868... Val Loss: 1.4417\nEpoch: 21/25... Step: 2820... Loss: 1.2481... Val Loss: 1.4416\nEpoch: 21/25... Step: 2830... Loss: 1.2601... Val Loss: 1.4349\nEpoch: 21/25... Step: 2840... Loss: 1.2107... Val Loss: 1.4351\nEpoch: 21/25... Step: 2850... Loss: 1.2282... Val Loss: 1.4346\nEpoch: 21/25... Step: 2860... Loss: 1.2155... Val Loss: 1.4402\nEpoch: 21/25... Step: 2870... Loss: 1.2486... Val Loss: 1.4328\nEpoch: 21/25... Step: 2880... Loss: 1.2339... Val Loss: 1.4387\nEpoch: 21/25... Step: 2890... Loss: 1.2224... Val Loss: 1.4369\nEpoch: 21/25... Step: 2900... Loss: 1.2122... Val Loss: 1.4365\nEpoch: 21/25... Step: 2910... Loss: 1.2491... Val Loss: 1.4351\nEpoch: 22/25... Step: 2920... Loss: 1.3364... Val Loss: 1.4448\nEpoch: 22/25... Step: 2930... Loss: 1.2539... Val Loss: 1.4384\nEpoch: 22/25... Step: 2940... Loss: 1.2569... Val Loss: 1.4379\nEpoch: 22/25... Step: 2950... Loss: 1.2700... Val Loss: 1.4349\nEpoch: 22/25... Step: 2960... Loss: 1.2138... Val Loss: 1.4438\nEpoch: 22/25... Step: 2970... Loss: 1.2141... Val Loss: 1.4363\nEpoch: 22/25... Step: 2980... Loss: 1.1981... Val Loss: 1.4295\nEpoch: 22/25... Step: 2990... Loss: 1.2109... Val Loss: 1.4387\nEpoch: 22/25... Step: 3000... Loss: 1.2125... Val Loss: 1.4455\nEpoch: 22/25... Step: 3010... Loss: 1.2211... Val Loss: 1.4422\nEpoch: 22/25... Step: 3020... Loss: 1.2401... Val Loss: 1.4350\nEpoch: 22/25... Step: 3030... Loss: 1.2145... Val Loss: 1.4374\nEpoch: 22/25... Step: 3040... Loss: 1.2043... Val Loss: 1.4321\nEpoch: 22/25... Step: 3050... Loss: 1.2439... Val Loss: 1.4347\nEpoch: 23/25... Step: 3060... Loss: 1.2187... Val Loss: 1.4354\nEpoch: 23/25... Step: 3070... Loss: 1.2355... Val Loss: 1.4308\nEpoch: 23/25... Step: 3080... Loss: 1.2222... Val Loss: 1.4275\nEpoch: 23/25... Step: 3090... Loss: 1.2106... Val Loss: 1.4258\nEpoch: 23/25... Step: 3100... Loss: 1.2015... Val Loss: 1.4323\nEpoch: 23/25... Step: 3110... Loss: 1.2201... Val Loss: 1.4265\nEpoch: 23/25... Step: 3120... Loss: 1.2387... Val Loss: 1.4280\nEpoch: 23/25... Step: 3130... Loss: 1.2235... Val Loss: 1.4288\nEpoch: 23/25... Step: 3140... Loss: 1.1802... Val Loss: 1.4384\nEpoch: 23/25... Step: 3150... Loss: 1.2053... Val Loss: 1.4421\nEpoch: 23/25... Step: 3160... Loss: 1.2361... Val Loss: 1.4380\nEpoch: 23/25... Step: 3170... Loss: 1.1957... Val Loss: 1.4420\nEpoch: 23/25... Step: 3180... Loss: 1.2096... Val Loss: 1.4335\nEpoch: 23/25... Step: 3190... Loss: 1.2164... Val Loss: 1.4270\nEpoch: 24/25... Step: 3200... Loss: 1.2240... Val Loss: 1.4290\nEpoch: 24/25... Step: 3210... Loss: 1.1946... Val Loss: 1.4249\nEpoch: 24/25... Step: 3220... Loss: 1.2416... Val Loss: 1.4305\nEpoch: 24/25... Step: 3230... Loss: 1.1843... Val Loss: 1.4326\nEpoch: 24/25... Step: 3240... Loss: 1.1684... Val Loss: 1.4237\nEpoch: 24/25... Step: 3250... Loss: 1.2296... Val Loss: 1.4280\nEpoch: 24/25... Step: 3260... Loss: 1.2377... Val Loss: 1.4252\nEpoch: 24/25... Step: 3270... Loss: 1.2198... Val Loss: 1.4314\nEpoch: 24/25... Step: 3280... Loss: 1.2116... Val Loss: 1.4320\nEpoch: 24/25... Step: 3290... Loss: 1.2027... Val Loss: 1.4268\nEpoch: 24/25... Step: 3300... Loss: 1.2086... Val Loss: 1.4298\nEpoch: 24/25... Step: 3310... Loss: 1.1980... Val Loss: 1.4386\nEpoch: 24/25... Step: 3320... Loss: 1.1918... Val Loss: 1.4318\nEpoch: 24/25... Step: 3330... Loss: 1.2409... Val Loss: 1.4289\nEpoch: 25/25... Step: 3340... Loss: 1.2129... Val Loss: 1.4239\nEpoch: 25/25... Step: 3350... Loss: 1.2122... Val Loss: 1.4228\nEpoch: 25/25... Step: 3360... Loss: 1.1933... Val Loss: 1.4243\nEpoch: 25/25... Step: 3370... Loss: 1.1921... Val Loss: 1.4313\nEpoch: 25/25... Step: 3380... Loss: 1.2025... Val Loss: 1.4262\nEpoch: 25/25... Step: 3390... Loss: 1.1884... Val Loss: 1.4257\nEpoch: 25/25... Step: 3400... Loss: 1.1995... Val Loss: 1.4216\nEpoch: 25/25... Step: 3410... Loss: 1.2172... Val Loss: 1.4277\nEpoch: 25/25... Step: 3420... Loss: 1.1930... Val Loss: 1.4299\nEpoch: 25/25... Step: 3430... Loss: 1.2057... Val Loss: 1.4189\nEpoch: 25/25... Step: 3440... Loss: 1.1882... Val Loss: 1.4310\nEpoch: 25/25... Step: 3450... Loss: 1.2035... Val Loss: 1.4357\nEpoch: 25/25... Step: 3460... Loss: 1.2075... Val Loss: 1.4333\nEpoch: 25/25... Step: 3470... Loss: 1.1989... Val Loss: 1.4222\n"
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_25_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime=\"The\", top_k=None, cuda=True):\n",
    "    if cuda: net.cuda()\n",
    "    net.eval()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    for i in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Anna.\n\nAnd the singly weathere were as though as he heard her husband he went\nup, and was so awkward to his brother, who sat surrounded by the sick\nman, and he could not have told some more, boining that the moter\nsincered and the sense of whom he had been a long while there\nshe had thought for the state. He was not a child. Before he stood\nbeginning to her that he was simply, what had been a party\nstill at the committee that was a sort of stelling on a lot that\ntricked at the sacious accesting on his singer with his humiliations\nwith her talking at the words.\n\nThe depression had been done, well, so he could not ask them at the same\ntime, before his beauty; to be still asterlious weare with his\nbrother that she went to his wife.\n\n\"I shall be, I can't go away,\" answered Levin, shaming her their head, the\ndaughter so as thinking to her, and the same talking of any official\nsignificance. And wish that she was said to him.\n\n\"Yes, it's all at announce in this candue, that's the most\ncondition of most still and more freedom! I am not time? We shall have the\nstation from the pastage as the children we must carry to the mors\nin a pititures, and as the care than only be the porter whomever to\ndecline it immense that anything, and then he's a submissive family.\nIt's a song in for the matter, there's the district of a country or\nall the presingentess there is a cross than out,\" he said.\n\n\"You can go to a mar for your son, and so he was, at this condition and son out\nof his commission in a storm. He did not like a little bill.\"\n\nAfter dreathing his face that he would go on to the coachman, and to\nher, as she could not be, and happened out to him as though he\nhad been an importance with his sense of the stals, a spenitule of a strange\nof cares and song of than eyes and he had been in a tentery feeling, that she\nhad said a little on her eyes, and he had suddenly been disprinced. The minuse\nof all had been this solution and holding happens of his\nwatch. They were a silence with her his\n"
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}